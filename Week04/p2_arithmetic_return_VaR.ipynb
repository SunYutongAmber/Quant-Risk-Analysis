{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2n/lm0km0mn4zlf79tvv2679h740000gn/T/ipykernel_53783/4066156773.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  output_dataframe[column] = returns[:, i]\n",
      "/var/folders/2n/lm0km0mn4zlf79tvv2679h740000gn/T/ipykernel_53783/4066156773.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  output_dataframe[column] = returns[:, i]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>SPY</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>BRK-B</th>\n",
       "      <th>...</th>\n",
       "      <th>CI</th>\n",
       "      <th>ETN</th>\n",
       "      <th>SLB</th>\n",
       "      <th>PGR</th>\n",
       "      <th>SCHW</th>\n",
       "      <th>LRCX</th>\n",
       "      <th>ZTS</th>\n",
       "      <th>C</th>\n",
       "      <th>BSX</th>\n",
       "      <th>AMT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-02</td>\n",
       "      <td>-0.010544</td>\n",
       "      <td>-0.013611</td>\n",
       "      <td>-0.016667</td>\n",
       "      <td>-0.002425</td>\n",
       "      <td>-0.020808</td>\n",
       "      <td>-0.017223</td>\n",
       "      <td>-0.025076</td>\n",
       "      <td>-0.016915</td>\n",
       "      <td>-0.016854</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001180</td>\n",
       "      <td>-0.010593</td>\n",
       "      <td>0.033107</td>\n",
       "      <td>-0.010428</td>\n",
       "      <td>-0.019242</td>\n",
       "      <td>-0.004236</td>\n",
       "      <td>-0.015244</td>\n",
       "      <td>0.001846</td>\n",
       "      <td>-0.012198</td>\n",
       "      <td>-0.026355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-06</td>\n",
       "      <td>-0.003773</td>\n",
       "      <td>-0.008215</td>\n",
       "      <td>-0.010974</td>\n",
       "      <td>-0.010980</td>\n",
       "      <td>-0.013336</td>\n",
       "      <td>-0.009643</td>\n",
       "      <td>0.015581</td>\n",
       "      <td>-0.011042</td>\n",
       "      <td>-0.003890</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004641</td>\n",
       "      <td>0.008449</td>\n",
       "      <td>-0.014118</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>-0.008019</td>\n",
       "      <td>-0.000892</td>\n",
       "      <td>-0.012695</td>\n",
       "      <td>-0.002717</td>\n",
       "      <td>0.013275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-07</td>\n",
       "      <td>0.017965</td>\n",
       "      <td>0.009254</td>\n",
       "      <td>0.019111</td>\n",
       "      <td>0.026723</td>\n",
       "      <td>0.018795</td>\n",
       "      <td>0.024717</td>\n",
       "      <td>0.033817</td>\n",
       "      <td>0.027912</td>\n",
       "      <td>0.016089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016652</td>\n",
       "      <td>0.020295</td>\n",
       "      <td>-0.008030</td>\n",
       "      <td>0.038537</td>\n",
       "      <td>0.018731</td>\n",
       "      <td>0.012279</td>\n",
       "      <td>0.022698</td>\n",
       "      <td>0.008503</td>\n",
       "      <td>0.026994</td>\n",
       "      <td>0.020930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-08</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>-0.009618</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>0.002626</td>\n",
       "      <td>0.020126</td>\n",
       "      <td>-0.009776</td>\n",
       "      <td>0.019598</td>\n",
       "      <td>-0.009595</td>\n",
       "      <td>0.008184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>0.013945</td>\n",
       "      <td>0.029951</td>\n",
       "      <td>0.015880</td>\n",
       "      <td>0.019083</td>\n",
       "      <td>0.016574</td>\n",
       "      <td>-0.011908</td>\n",
       "      <td>0.026116</td>\n",
       "      <td>0.029901</td>\n",
       "      <td>0.008362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-09</td>\n",
       "      <td>0.015535</td>\n",
       "      <td>0.018840</td>\n",
       "      <td>0.022977</td>\n",
       "      <td>0.026575</td>\n",
       "      <td>0.028377</td>\n",
       "      <td>0.020945</td>\n",
       "      <td>0.036023</td>\n",
       "      <td>0.021568</td>\n",
       "      <td>0.008576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007327</td>\n",
       "      <td>0.017244</td>\n",
       "      <td>0.038774</td>\n",
       "      <td>-0.004179</td>\n",
       "      <td>0.018863</td>\n",
       "      <td>0.026460</td>\n",
       "      <td>0.036721</td>\n",
       "      <td>0.015431</td>\n",
       "      <td>0.005385</td>\n",
       "      <td>-0.000306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>2023-09-18</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.016913</td>\n",
       "      <td>-0.003513</td>\n",
       "      <td>-0.002920</td>\n",
       "      <td>0.001503</td>\n",
       "      <td>0.005895</td>\n",
       "      <td>-0.033201</td>\n",
       "      <td>0.004772</td>\n",
       "      <td>0.006986</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007485</td>\n",
       "      <td>0.006938</td>\n",
       "      <td>0.010399</td>\n",
       "      <td>0.013118</td>\n",
       "      <td>-0.006183</td>\n",
       "      <td>0.020125</td>\n",
       "      <td>-0.003329</td>\n",
       "      <td>-0.001639</td>\n",
       "      <td>0.001890</td>\n",
       "      <td>-0.003386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>2023-09-19</td>\n",
       "      <td>-0.002074</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>-0.001246</td>\n",
       "      <td>-0.016788</td>\n",
       "      <td>-0.010144</td>\n",
       "      <td>-0.001230</td>\n",
       "      <td>0.004599</td>\n",
       "      <td>-0.000936</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002453</td>\n",
       "      <td>-0.013644</td>\n",
       "      <td>-0.012743</td>\n",
       "      <td>0.013589</td>\n",
       "      <td>-0.002247</td>\n",
       "      <td>-0.016519</td>\n",
       "      <td>0.012970</td>\n",
       "      <td>0.000938</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>-0.012087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>-0.009193</td>\n",
       "      <td>-0.019992</td>\n",
       "      <td>-0.023977</td>\n",
       "      <td>-0.017002</td>\n",
       "      <td>-0.029435</td>\n",
       "      <td>-0.031150</td>\n",
       "      <td>-0.014672</td>\n",
       "      <td>-0.030541</td>\n",
       "      <td>-0.009879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009450</td>\n",
       "      <td>-0.006986</td>\n",
       "      <td>-0.010591</td>\n",
       "      <td>0.001544</td>\n",
       "      <td>-0.018361</td>\n",
       "      <td>-0.010062</td>\n",
       "      <td>-0.002748</td>\n",
       "      <td>-0.008903</td>\n",
       "      <td>0.020177</td>\n",
       "      <td>0.000282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>2023-09-21</td>\n",
       "      <td>-0.016528</td>\n",
       "      <td>-0.008889</td>\n",
       "      <td>-0.003866</td>\n",
       "      <td>-0.044053</td>\n",
       "      <td>-0.028931</td>\n",
       "      <td>-0.024675</td>\n",
       "      <td>-0.026239</td>\n",
       "      <td>-0.023999</td>\n",
       "      <td>-0.009651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012216</td>\n",
       "      <td>-0.018635</td>\n",
       "      <td>-0.016223</td>\n",
       "      <td>-0.002032</td>\n",
       "      <td>-0.011646</td>\n",
       "      <td>-0.013686</td>\n",
       "      <td>-0.026725</td>\n",
       "      <td>-0.013948</td>\n",
       "      <td>-0.002403</td>\n",
       "      <td>-0.045601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>2023-09-22</td>\n",
       "      <td>-0.002249</td>\n",
       "      <td>0.004945</td>\n",
       "      <td>-0.007887</td>\n",
       "      <td>-0.001624</td>\n",
       "      <td>0.014457</td>\n",
       "      <td>-0.001457</td>\n",
       "      <td>-0.042315</td>\n",
       "      <td>-0.000837</td>\n",
       "      <td>-0.008588</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004814</td>\n",
       "      <td>0.009542</td>\n",
       "      <td>0.003740</td>\n",
       "      <td>0.006039</td>\n",
       "      <td>-0.015354</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>-0.018940</td>\n",
       "      <td>-0.006856</td>\n",
       "      <td>-0.018368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>265 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date       SPY      AAPL      MSFT      AMZN      NVDA     GOOGL  \\\n",
       "0    2022-09-02 -0.010544 -0.013611 -0.016667 -0.002425 -0.020808 -0.017223   \n",
       "1    2022-09-06 -0.003773 -0.008215 -0.010974 -0.010980 -0.013336 -0.009643   \n",
       "2    2022-09-07  0.017965  0.009254  0.019111  0.026723  0.018795  0.024717   \n",
       "3    2022-09-08  0.006536 -0.009618  0.001666  0.002626  0.020126 -0.009776   \n",
       "4    2022-09-09  0.015535  0.018840  0.022977  0.026575  0.028377  0.020945   \n",
       "..          ...       ...       ...       ...       ...       ...       ...   \n",
       "260  2023-09-18  0.000586  0.016913 -0.003513 -0.002920  0.001503  0.005895   \n",
       "261  2023-09-19 -0.002074  0.006181 -0.001246 -0.016788 -0.010144 -0.001230   \n",
       "262  2023-09-20 -0.009193 -0.019992 -0.023977 -0.017002 -0.029435 -0.031150   \n",
       "263  2023-09-21 -0.016528 -0.008889 -0.003866 -0.044053 -0.028931 -0.024675   \n",
       "264  2023-09-22 -0.002249  0.004945 -0.007887 -0.001624  0.014457 -0.001457   \n",
       "\n",
       "         TSLA      GOOG     BRK-B  ...        CI       ETN       SLB  \\\n",
       "0   -0.025076 -0.016915 -0.016854  ... -0.001180 -0.010593  0.033107   \n",
       "1    0.015581 -0.011042 -0.003890  ... -0.004641  0.008449 -0.014118   \n",
       "2    0.033817  0.027912  0.016089  ...  0.016652  0.020295 -0.008030   \n",
       "3    0.019598 -0.009595  0.008184  ...  0.002448  0.013945  0.029951   \n",
       "4    0.036023  0.021568  0.008576  ...  0.007327  0.017244  0.038774   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "260 -0.033201  0.004772  0.006986  ...  0.007485  0.006938  0.010399   \n",
       "261  0.004599 -0.000936  0.000135  ... -0.002453 -0.013644 -0.012743   \n",
       "262 -0.014672 -0.030541 -0.009879  ...  0.009450 -0.006986 -0.010591   \n",
       "263 -0.026239 -0.023999 -0.009651  ...  0.012216 -0.018635 -0.016223   \n",
       "264 -0.042315 -0.000837 -0.008588  ... -0.004814  0.009542  0.003740   \n",
       "\n",
       "          PGR      SCHW      LRCX       ZTS         C       BSX       AMT  \n",
       "0   -0.010428 -0.019242 -0.004236 -0.015244  0.001846 -0.012198 -0.026355  \n",
       "1    0.000572  0.001848 -0.008019 -0.000892 -0.012695 -0.002717  0.013275  \n",
       "2    0.038537  0.018731  0.012279  0.022698  0.008503  0.026994  0.020930  \n",
       "3    0.015880  0.019083  0.016574 -0.011908  0.026116  0.029901  0.008362  \n",
       "4   -0.004179  0.018863  0.026460  0.036721  0.015431  0.005385 -0.000306  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "260  0.013118 -0.006183  0.020125 -0.003329 -0.001639  0.001890 -0.003386  \n",
       "261  0.013589 -0.002247 -0.016519  0.012970  0.000938  0.000566 -0.012087  \n",
       "262  0.001544 -0.018361 -0.010062 -0.002748 -0.008903  0.020177  0.000282  \n",
       "263 -0.002032 -0.011646 -0.013686 -0.026725 -0.013948 -0.002403 -0.045601  \n",
       "264  0.006039 -0.015354  0.014286  0.000283 -0.018940 -0.006856 -0.018368  \n",
       "\n",
       "[265 rows x 102 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def returns(prices, method=\"discrete\", date_column=\"date\"):\n",
    "    columns = list(prices.columns)\n",
    "    columns.remove(date_column)\n",
    "    \n",
    "    num_columns = len(columns) - 1\n",
    "    \n",
    "    price_data = prices[columns].values\n",
    "    num_rows, num_cols = price_data.shape\n",
    "    returns = np.empty((num_rows - 1, num_cols), dtype=float)\n",
    "    \n",
    "    method = method.lower()\n",
    "    if method == \"discrete\" or method == \"log\":\n",
    "        for i in range(num_rows - 1):\n",
    "            for j in range(num_cols):\n",
    "                returns[i, j] = price_data[i + 1, j] / price_data[i, j]\n",
    "        \n",
    "        if method == \"discrete\":\n",
    "            returns = returns - 1.0\n",
    "        else:\n",
    "            returns = np.log(returns)\n",
    "    elif method == \"classic\":\n",
    "        for i in range(num_rows - 1):\n",
    "            for j in range(num_cols):\n",
    "                returns[i, j] = price_data[i + 1, j] - price_data[i, j]\n",
    "    else:\n",
    "        raise ValueError(f\"Method: {method} must be in ('log', 'discrete', 'classic')\")\n",
    "    \n",
    "    dates = prices.iloc[1:, prices.columns.get_loc(date_column)].tolist()\n",
    "    \n",
    "    output_dataframe = pd.DataFrame({date_column: dates})\n",
    "    for i, column in enumerate(columns):\n",
    "        output_dataframe[column] = returns[:, i]\n",
    "    \n",
    "    return output_dataframe\n",
    "\n",
    "# Example usage\n",
    "daily_prices = pd.read_csv(\"DailyPrices.csv\")\n",
    "daily_returns = returns(daily_prices, method=\"discrete\", date_column=\"Date\")\n",
    "meta_returns = daily_returns['META'].values\n",
    "meta_return_mean = meta_returns.mean()\n",
    "normalized_meta_returns = meta_returns - meta_return_mean\n",
    "daily_returns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Calculate VaR using a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VaR for Normal Distribution model is 0.07907071888888416\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.01\n",
    "sigma = normalize_meta_return.std()\n",
    "normal_distribution_model = np.random.normal(0, sigma , 1000)\n",
    "VaR_normal_distribution = -np.quantile(normal_distribution_model, alpha)\n",
    "print(\"VaR for Normal Distribution model is\", VaR_normal_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Distribution with Exponentially Weighted variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VaR for normal distribution with exponentially weighted variance is: 0.029930783600041692\n"
     ]
    }
   ],
   "source": [
    "lambda_param = 0.94\n",
    "total_weights = 0\n",
    "num_returns = len(normalized_meta_returns)\n",
    "weights = np.zeros(num_returns)\n",
    "\n",
    "# Calculate weights using exponentially weighted method\n",
    "for i in range(num_returns):\n",
    "    weights[i] = (1 - lambda_param) * lambda_param ** (num_returns - i - 1)\n",
    "    total_weights += weights[i]\n",
    "\n",
    "# Normalize the weights\n",
    "for i in range(num_returns):\n",
    "    weights[i] = weights[i] / total_weights\n",
    "\n",
    "# Calculate the exponentially weighted standard deviation\n",
    "sigma_ew = np.sqrt((normalized_meta_returns * weights).T @ normalized_meta_returns)\n",
    "\n",
    "# Generate random returns with normal distribution using the calculated standard deviation\n",
    "num_simulations = 10000\n",
    "random_returns = np.random.normal(0, sigma_ew, num_simulations)\n",
    "\n",
    "# Calculate Value at Risk (VaR)\n",
    "alpha = 0.05\n",
    "VaR_ew = -np.quantile(random_returns, alpha)\n",
    "\n",
    "print(\"VaR for normal distribution with exponentially weighted variance is:\", VaR_ew)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE fitted T distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VaR for maximum likelihood estimated T distribution is: 0.04375001669592293\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def calculate_negative_log_likelihood(params, data):\n",
    "    degrees_of_freedom, location, scale = params\n",
    "    log_likelihood = np.sum(stats.t.logpdf(data, df=degrees_of_freedom, loc=location, scale=scale))\n",
    "    return -log_likelihood\n",
    "\n",
    "# Initial parameters for optimization\n",
    "initial_parameters = [3, 0, 1]\n",
    "\n",
    "# Minimize the negative log likelihood\n",
    "result = minimize(calculate_negative_log_likelihood, initial_parameters, args=(normalize_meta_return,), method='Nelder-Mead')\n",
    "\n",
    "# Extract estimated parameters\n",
    "estimated_degrees_of_freedom, estimated_location, estimated_scale = result.x\n",
    "\n",
    "# Generate random variates from the fitted t-distribution\n",
    "num_samples = 10000\n",
    "MLE_fitted_t_model = stats.t.rvs(df=estimated_degrees_of_freedom, loc=estimated_location, scale=estimated_scale, size=num_samples)\n",
    "\n",
    "# Calculate Value at Risk (VaR)\n",
    "alpha = 0.05\n",
    "VaR_MLE = -np.quantile(MLE_fitted_t_model, alpha)\n",
    "\n",
    "print(\"VaR for maximum likelihood estimated T distribution is:\", VaR_MLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AR（1）Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VaR for ARIMA(1, 0, 0) model is: 0.0014033794443858542\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Fit ARIMA model\n",
    "arima_model = sm.tsa.ARIMA(normalize_meta_return, order=(1, 0, 0))\n",
    "arima_results = arima_model.fit().params\n",
    "\n",
    "# Number of simulations\n",
    "num_simulations = 10000\n",
    "\n",
    "# Generate random errors from a normal distribution\n",
    "random_errors = np.random.normal(0, arima_results[2], num_simulations)\n",
    "\n",
    "# Generate returns using the ARIMA(1, 0, 0) model\n",
    "returns = np.zeros(num_simulations)\n",
    "last_return = normalize_meta_return[-1]\n",
    "for i in range(num_simulations):\n",
    "    returns[i] = arima_results[0] + last_return * arima_results[1] + random_errors[i]\n",
    "\n",
    "# Calculate Value at Risk (VaR)\n",
    "alpha = 0.05\n",
    "VaR_arima = -np.quantile(returns, alpha)\n",
    "\n",
    "print(\"VaR for ARIMA(1, 0, 0) model is:\", VaR_arima)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VaR with historical simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAR for Historic Simulation is: 0.03939050784430346\n"
     ]
    }
   ],
   "source": [
    "# Number of simulations\n",
    "num_simulations = 10000\n",
    "rsim = np.random.choice(normalize_meta_return,num_simulations)\n",
    "VaR = -np.quantile(rsim,alpha)\n",
    "\n",
    "print(\"VAR for Historic Simulation is: {}\".format(VaR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
